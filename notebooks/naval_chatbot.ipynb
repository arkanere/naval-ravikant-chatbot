{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports cell\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key from environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not openai.api_key or openai.api_key == \"your-openai-api-key\":\n",
    "    raise ValueError(\"Please set your OpenAI API key in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load the book\n",
    "# pdf_path = \"data/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf\"  # Update with your actual file path\n",
    "pdf_path = \"../data/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf\"\n",
    "\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Preview the first 1000 characters\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    # Remove excessive newlines and whitespace\n",
    "    cleaned_text = \" \".join(text.split())\n",
    "    # Add more preprocessing as needed\n",
    "    return cleaned_text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split text into manageable chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Clean and chunk the text\n",
    "cleaned_text = preprocess_text(raw_text)\n",
    "text_chunks = chunk_text(cleaned_text)\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Total chunks created: {len(text_chunks)}\")\n",
    "print(f\"Average chunk length: {sum(len(chunk) for chunk in text_chunks) / len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    \"\"\"Create embeddings for text chunks using BAAI/bge-large-en model\"\"\"\n",
    "    # Initialize the HuggingFace embeddings with BAAI/bge-large-en\n",
    "    model_name = \"BAAI/bge-large-en\"\n",
    "    model_kwargs = {'device': 'cpu'}  # Use 'cuda' if you have GPU\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    # Create the vector store with FAISS\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Create vector store\n",
    "vector_store = create_embeddings(text_chunks)\n",
    "\n",
    "# Optional: Show a message when embeddings are complete\n",
    "print(f\"Vector store created successfully with {len(text_chunks)} chunks using BAAI/bge-large-en model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store to disk for future use\n",
    "import pickle\n",
    "\n",
    "def save_vector_store(vector_store, file_path=\"naval_vector_store.pkl\"):\n",
    "    \"\"\"Save vector store to disk\"\"\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "\n",
    "def load_vector_store(file_path=\"naval_vector_store.pkl\"):\n",
    "    \"\"\"Load vector store from disk\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        vector_store = pickle.load(f)\n",
    "    return vector_store\n",
    "\n",
    "# Save for future use\n",
    "save_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(query, vector_store, k=5):\n",
    "    \"\"\"Retrieve the most relevant chunks for a query\"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    contexts = [doc.page_content for doc in docs]\n",
    "    return contexts\n",
    "\n",
    "# Test the retrieval system\n",
    "test_query = \"What does Naval say about fundamental delusion?\"\n",
    "relevant_contexts = get_relevant_context(test_query, vector_store)\n",
    "\n",
    "# Display the first retrieved context\n",
    "print(relevant_contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_contexts(contexts, query):\n",
    "    \"\"\"Rank contexts by relevance to query\"\"\"\n",
    "    # This could be enhanced with a more sophisticated ranking algorithm\n",
    "    return contexts\n",
    "\n",
    "def format_contexts(contexts):\n",
    "    \"\"\"Format the contexts for inclusion in the prompt\"\"\"\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    return f\"RELEVANT PASSAGES FROM 'THE ALMANACK OF NAVAL RAVIKANT':\\n\\n{formatted_context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt():\n",
    "    \"\"\"Create the system prompt that defines the chatbot's behavior\"\"\"\n",
    "    return \"\"\"You are a conversational AI assistant who has thoroughly studied 'The Almanack of Naval Ravikant'. \n",
    "    \n",
    "    Your purpose is to engage in conversations about Naval's philosophy, perspectives, and wisdom as presented in the book.\n",
    "    \n",
    "    Guidelines:\n",
    "    1. Base your responses primarily on the provided context from the book.\n",
    "    2. Maintain Naval's distinctive voice and communication style.\n",
    "    3. Use Naval's actual quotes when available and appropriate.\n",
    "    4. Be honest about the limitations of your knowledge. If a user asks about something not covered in the book, acknowledge this.\n",
    "    5. Don't make up quotes or attribute ideas to Naval that aren't supported by the book.\n",
    "    6. Keep responses concise and clear, similar to Naval's communication style.\n",
    "    \n",
    "    Remember, your goal is to accurately represent Naval's ideas as presented in 'The Almanack of Naval Ravikant', not to provide general wisdom or advice.\"\"\"\n",
    "\n",
    "def create_prompt(query, contexts, chat_history):\n",
    "    \"\"\"Create a prompt for the GPT-4 model\"\"\"\n",
    "    system_prompt = create_system_prompt()\n",
    "    formatted_context = format_contexts(contexts)\n",
    "    \n",
    "    # Format chat history\n",
    "    formatted_history = \"\"\n",
    "    if chat_history:\n",
    "        formatted_history = \"PREVIOUS CONVERSATION:\\n\"\n",
    "        for q, a in chat_history:\n",
    "            formatted_history += f\"User: {q}\\nNaval Bot: {a}\\n\\n\"\n",
    "    \n",
    "    user_prompt = f\"\"\"QUERY: {query}\n",
    "\n",
    "{formatted_context}\n",
    "\n",
    "Based on the provided passages from Naval Ravikant's book, please respond to the query in Naval's voice and perspective.\"\"\"\n",
    "    \n",
    "    return system_prompt, user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4_response(system_prompt, user_prompt):\n",
    "    \"\"\"Get a response from GPT-4 API\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "5.3 Create Conversation Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavalChatbot:\n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def ask(self, query, k=5):\n",
    "        \"\"\"Process a user query and return a response\"\"\"\n",
    "        # Get relevant contexts\n",
    "        contexts = get_relevant_context(query, self.vector_store, k=k)\n",
    "        \n",
    "        # Create prompt\n",
    "        system_prompt, user_prompt = create_prompt(query, contexts, self.chat_history)\n",
    "        \n",
    "        # Get response\n",
    "        response = get_gpt4_response(system_prompt, user_prompt)\n",
    "        \n",
    "        # Update chat history\n",
    "        self.chat_history.append((query, response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset the conversation history\"\"\"\n",
    "        self.chat_history = []\n",
    "\n",
    "# Initialize chatbot\n",
    "naval_bot = NavalChatbot(vector_store)\n",
    "\n",
    "# Test the chatbot\n",
    "response = naval_bot.ask(\"What does Naval believe about happiness?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_ui():\n",
    "    \"\"\"Create an interactive chat UI using IPython widgets\"\"\"\n",
    "    # Create the widgets\n",
    "    output = widgets.Output(layout={'border': '1px solid #666', 'min_height': '400px', 'width': '100%'})\n",
    "    input_box = widgets.Text(placeholder='Type your message here...', layout={'width': '80%'})\n",
    "    send_button = widgets.Button(description='Send', layout={'width': '19%'})\n",
    "    reset_button = widgets.Button(description='Reset Chat', layout={'width': '100%'})\n",
    "    \n",
    "    # Layout the widgets\n",
    "    input_area = widgets.HBox([input_box, send_button])\n",
    "    container = widgets.VBox([output, input_area, reset_button])\n",
    "    \n",
    "    # Define callback functions\n",
    "    def send_message(_):\n",
    "        query = input_box.value\n",
    "        input_box.value = ''\n",
    "        \n",
    "        with output:\n",
    "            # User message with higher contrast\n",
    "            display(HTML(f\"<div style='margin: 5px; padding: 5px; background-color: #4a4a4a; color: white; border-radius: 5px;'><b>You:</b> {query}</div>\"))\n",
    "            \n",
    "            # Get response from bot\n",
    "            response = naval_bot.ask(query)\n",
    "            \n",
    "            # Bot message with higher contrast\n",
    "            display(HTML(f\"<div style='margin: 5px; padding: 5px; background-color: #1a5276; color: white; border-radius: 5px;'><b>Naval Bot:</b> {response}</div>\"))\n",
    "    \n",
    "    def reset_chat(_):\n",
    "        naval_bot.reset_conversation()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            display(HTML(\"<div style='margin: 5px; padding: 5px; background-color: #1a5276; color: white; border-radius: 5px;'><b>Naval Bot:</b> Hello! I'm Naval Bot. Ask me anything about Naval Ravikant's philosophy from 'The Almanack of Naval Ravikant'.</div>\"))\n",
    "    \n",
    "    # Connect callbacks to widgets\n",
    "    send_button.on_click(send_message)\n",
    "    input_box.on_submit(send_message)\n",
    "    reset_button.on_click(reset_chat)\n",
    "    \n",
    "    # Display initial message\n",
    "    with output:\n",
    "        display(HTML(\"<div style='margin: 5px; padding: 5px; background-color: #1a5276; color: white; border-radius: 5px;'><b>Naval Bot:</b> Hello! I'm Naval Bot. Ask me anything about Naval Ravikant's philosophy from 'The Almanack of Naval Ravikant'.</div>\"))\n",
    "    \n",
    "    return container\n",
    "\n",
    "# Create and display the chat UI\n",
    "chat_ui = create_chat_ui()\n",
    "display(chat_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(test_questions, naval_bot):\n",
    "    \"\"\"Evaluate the quality of responses for a set of test questions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        response = naval_bot.ask(question)\n",
    "        \n",
    "        # Add results to list\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"response\": response,\n",
    "            # You could add manual or automated evaluation metrics here\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define test questions\n",
    "test_questions = [\n",
    "    \"What does Naval say about wealth creation?\",\n",
    "    \"How does Naval define happiness?\",\n",
    "    \"What are Naval's thoughts on reading?\",\n",
    "    \"What does Naval believe about the meaning of life?\",\n",
    "    \"How does Naval approach decision-making?\"\n",
    "]\n",
    "\n",
    "# Reset the chat history before evaluation\n",
    "naval_bot.reset_conversation()\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_responses(test_questions, naval_bot)\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results['response'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adjust the number of chunks retrieved based on evaluation\n",
    "def optimize_chunk_retrieval(naval_bot, test_questions, k_values=[3, 5, 7, 10]):\n",
    "    \"\"\"Optimize the number of chunks to retrieve\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        naval_bot.reset_conversation()\n",
    "        \n",
    "        # Test with different k values\n",
    "        for question in test_questions:\n",
    "            response = naval_bot.ask(question, k=k)\n",
    "            results.append({\n",
    "                \"k\": k,\n",
    "                \"question\": question,\n",
    "                \"response\": response\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run optimization (this would typically require manual review of results)\n",
    "# optimization_results = optimize_chunk_retrieval(naval_bot, test_questions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate all components into a single function\n",
    "def launch_naval_chatbot():\n",
    "    \"\"\"Launch the complete Naval Ravikant chatbot\"\"\"\n",
    "    # Load the vector store (or create it if not available)\n",
    "    try:\n",
    "        vector_store = load_vector_store(\"models/naval_vector_store.pkl\")\n",
    "        print(\"Loaded existing vector store.\")\n",
    "    except:\n",
    "        print(\"Creating new vector store...\")\n",
    "        # Load and process the book\n",
    "        pdf_path = \"data/almanack_of_naval_ravikant.pdf\"  # Update with your file path\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        cleaned_text = preprocess_text(raw_text)\n",
    "        text_chunks = chunk_text(cleaned_text)\n",
    "        \n",
    "        # Create embeddings\n",
    "        vector_store = create_embeddings(text_chunks)\n",
    "        save_vector_store(vector_store, \"models/naval_vector_store.pkl\")\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    naval_bot = NavalChatbot(vector_store)\n",
    "    \n",
    "    # Display chat UI\n",
    "    chat_ui = create_chat_ui()\n",
    "    display(chat_ui)\n",
    "    \n",
    "    return naval_bot\n",
    "\n",
    "# Launch the chatbot\n",
    "naval_bot = launch_naval_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
